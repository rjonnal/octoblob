





# standard imports:
import numpy as np
from matplotlib import pyplot as plt



frame_index = 0

# Let's define a function to make pulling individual frames from the UNP file easy.
# We'll require a filename, a frame index, and a volume index. The volume index hasn't
# been relevant for ORG imaging since we aren't collecting multiple volumes, and the
# UNP files contain just one "volume", which actually consists of serial B-scans; thus
# we provide a default volume index of 0.
def get_frame(filename,frame_index,volume_index=0):
    
    # we want to read data into the numpy datatype uint16
    dtype = np.uint16
    # put the XML params into this function for safety:
    volume_index = 0
    bytes_per_pixel = 2
    n_depth = 1536
    n_fast = 250
    n_slow = 400

    # Calculate the entry point into the file:
    pixels_per_frame = n_depth * n_fast
    pixels_per_volume = pixels_per_frame * n_slow
    bytes_per_volume = pixels_per_volume * bytes_per_pixel
    bytes_per_frame = pixels_per_frame * bytes_per_pixel
    position = volume_index * bytes_per_volume + frame_index * bytes_per_frame
    
    # Use python open and seek commands in conjuction with numpy's fromfile command to read the frame:
    with open(filename,'rb') as fid:
        fid.seek(position,0)
        frame = np.fromfile(fid,dtype=dtype,count=pixels_per_frame)
    
    # For whatever reason the raw data are left shifted by 4 bits (i.e., they occupy the first 12 of 16 bits rather than the last 12, i.e. they are all 16x too large).
    bit_shift_right = 4
    frame = np.right_shift(frame,bit_shift_right)
    
    # Now we need to reshape the one-dimensional result of fromfile into a 2D array
    frame = np.reshape(frame,(n_fast,n_depth))
    
    # By convention we always put k or depth in the first dimension, so let's transpose:
    frame = frame.T

    return frame


# get the zeroth frame:
filename = '16_24_48.unp'
frame = get_frame(filename,0)
# Now show the frame:
plt.imshow(frame,aspect='auto')
plt.show()






plt.figure()
plt.imshow(frame,interpolation='none',aspect='auto')
plt.ylim((70,110))
plt.title('FBG not aligned')
plt.show()






# a function for aligning the spectra to the FBG trough
def fbg_align(spectra,fbg_max_index=150,fbg_region_correlation_threshold = 0.9):
    # crop the frame to the FBG region
    f = spectra[:fbg_max_index,:].copy()

    # group the spectra by amount of shift
    # this step avoids having to perform cross-correlation operations on every
    # spectrum; first, we group them by correlation with one another
    # make a list of spectra to group
    to_do = list(range(f.shape[1]))
    # make a list for the groups of similarly shifted spectra
    groups = []
    ref = 0

    # while there are spectra left to group, do the following loop:
    while(True):
        groups.append([ref])
        to_do.remove(ref)
        for tar in to_do:
            c = np.corrcoef(f[:,ref],f[:,tar])[0,1]
            if c>fbg_region_correlation_threshold:
                groups[-1].append(tar)
                to_do.remove(tar)
        if len(to_do)==0:
            break
        ref = to_do[0]

    subframes = []
    for g in groups:
        subf = f[:,g]
        subframes.append(subf)

    # now decide how to shift the groups of spectra by cross-correlating their means
    # we'll use the first group as the reference group:
    group_shifts = [0]
    ref = np.mean(subframes[0],axis=1)
    # now, iterate through the other groups, compute their means, and cross-correlate
    # with the reference. keep track of the cross-correlation peaks in the list group_shifts
    for taridx in range(1,len(subframes)):
        tar = np.mean(subframes[taridx],axis=1)
        xc = np.fft.ifft(np.fft.fft(ref)*np.fft.fft(tar).conj())
        shift = np.argmax(xc)
        if shift>len(xc)//2:
            shift = shift-len(xc)
        group_shifts.append(shift)

    # now, use the groups and the group_shifts to shift all of the spectra according to their
    # group membership:
    for g,s in zip(groups,group_shifts):
        for idx in g:
            spectra[:,idx] = np.roll(spectra[:,idx],s)
            f[:,idx] = np.roll(f[:,idx],s)

    return spectra

# make a copy of the unaligned frame for reference
# do this by making a new array, i.e. this:
# unaligned = frame
# will not work since both variables unaligned and frame refer to the same array
unaligned = np.zeros(frame.shape)
unaligned[:] = frame[:]
    

frame = fbg_align(frame)
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.imshow(unaligned,aspect='auto',interpolation='none')
plt.title('unaligned')
plt.ylim((70,110))
plt.subplot(1,2,2)
plt.imshow(frame,aspect='auto',interpolation='none')
plt.title('FBG aligned')
plt.ylim((70,110))
plt.show()






# average the spectra and plot the resulting DC estimate
DC = np.mean(frame,axis=1)
plt.plot(DC)
plt.show()





# Plot the single-frame DC estimate along with one of the spectra, offset for visibility
plt.figure()
plt.subplot(1,2,1)
plt.plot(DC)
plt.plot(frame[:,100]+200)
# Plot a 20-frame DC estimate along with one of the spectra:
frames_20 = [get_frame(filename,k) for k in range(20)] # use a list comprehension to make a list of 20 frames
frames_20 = np.hstack(frames_20) # turn the list of 20 frames into a large frame of size 1536 x 5000 by concatenation
DC_20 = np.mean(frames_20,axis=1)
plt.subplot(1,2,2)
plt.plot(DC_20)
plt.plot(frame[:,100]+200)
plt.show()
    





# Plot the single-frame DC and 20-frame DC together, zoomed in on a small region:
plt.plot(DC[450:550],label='1-frame DC')
plt.plot(DC_20[450:550],label='20-frame DC')
plt.legend()
plt.show()








# DC-subtract through array broadcasting with required transpose

# We can't do this:
# frame = frame - DC
# because the trailing dimensions of frame and DC disagree.
# Instead, transpose, subtract, and transpose back:
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.imshow(frame,aspect='auto')
plt.colorbar()

frame = (frame.T - DC).T
plt.subplot(1,2,2)
plt.imshow(frame,aspect='auto')
plt.colorbar()
plt.show()





def crop_spectra(spectra):
    k_crop_1 = 100
    k_crop_2 = 1490
    return spectra[k_crop_1:k_crop_2,:]

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.imshow(frame,aspect='auto')
plt.title('before cropping')

frame = crop_spectra(frame)

plt.subplot(1,2,2)
plt.imshow(frame,aspect='auto')
plt.title('after cropping')





def dB(x):
    return 20*np.log10(x)

def dispersion_compensate(spectra,coefficients):
    # If all coefficients are 0, return the spectra w/o further computation:
    if not any(coefficients):
        return spectra

    # the coefficients passed into this function are just the 3rd and 2nd order ones; we
    # add zeros so that we can use convenience functions like np.polyval that handle the
    # algebra; the input coefficients are [dc3,dc2], either a list or numpy array;
    # cast as a list to be on the safe side.
    coefs = list(coefficients) + [0.0,0.0]

    # now coefs is a 4-item list: [dc3,dc2,0.0,0.0]
    
    # define index x:
    x = np.arange(1,spectra.shape[0]+1)

    # if we want to avoid using polyval, we can explicitly evaluate the polynomial:
    # evaluate our polynomial on index x; if we do it this way, we need not append
    # zeroes to coefs above
    # phase_polynomial = coefs[0]*x**3 + coefs[1]*x**2

    # actually it's simpler to use polyval, which is why we appended the zeros to
    # the input coefficients--polyval infers the order of the polynomial from the
    # number of values in the list/array:
    phase_polynomial = np.polyval(coefs,x)
    
    # define the phasor and multiply by spectra using broadcasting:
    dechirping_phasor = np.exp(-1j*phase_polynomial)

    # A topic for future discussion is whether or not to include the next line;
    # for the time being it does not appear to make much of a difference, but
    # in theory I think it should be there.
    #spectra = spectra + 1j*sps.hilbert(spectra,axis=0)
    dechirped = (spectra.T*dechirping_phasor).T
    return dechirped


dispersion_coefficients = [-1.4e-8,-4.6e-6]

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.imshow(dB(np.abs(np.fft.fft(frame,axis=0))),cmap='gray',aspect='auto',clim=(45,90))
plt.ylim((1300,1100))
frame = dispersion_compensate(frame,dispersion_coefficients)
plt.subplot(1,2,2)
plt.imshow(dB(np.abs(np.fft.fft(frame,axis=0))),cmap='gray',aspect='auto',clim=(45,90))
plt.ylim((1300,1100))
plt.show()





## Let's use the steps above to create a convenience function for getting B-scans from a UNP file:

def get_bscan(filename,dispersion_coefficients,bscan_index,volume_index=0,N_frames_for_DC=20):
    frame = get_frame(filename,bscan_index,volume_index)
    frame = fbg_align(frame)
    dc_frames = [get_frame(filename,k) for k in range(N_frames_for_DC)] # use a list comprehension to make a list of 20 frames
    dc_frames = np.hstack(dc_frames) # turn the list of 20 frames into a large frame of size 1536 x 5000 by concatenation
    DC = np.mean(dc_frames,axis=1)
    frame = (frame.T - DC).T
    frame = crop_spectra(frame)
    frame = dispersion_compensate(frame,dispersion_coefficients)
    bscan = np.fft.fft(frame,axis=0)
    bscan = bscan[1100:1300,:]
    return bscan








# a cross-correlation function for 1D vectors:
def xcorr(a,b):
    return np.real(np.fft.ifft(np.fft.fft(a)*np.conj(np.fft.fft(b))))

dispersion_coefficients = [-1.4e-8,-4.6e-6]
filename = '16_24_48.unp'
starting_bscan = 100
block_size = 3

bscans = [get_bscan(filename,dispersion_coefficients,k) for k in range(starting_bscan,starting_bscan+block_size)]

plt.figure(figsize=(12,4))
for idx,bscan in enumerate(bscans):
    plt.subplot(1,3,idx+1)
    plt.imshow(dB(np.abs(bscan)),clim=(40,90),cmap='gray')
    plt.title('B-scan %d'%idx)
plt.show()

n_depth,n_fast = bscans[0].shape


plt.figure(figsize=(12,2))
#print('reference_index','\t','shifted_sister_count','\t','reference_fast_scan_location','\t','winning dx','\t','correlations')
for reference_index in range(block_size-1):
    target_index = reference_index + 1
    peaks = []
    shifted_sister_count = 0
    for x in range(1,n_fast-1):
        ref_ascan = bscans[reference_index][:,x]
        correlations = []
        for dx in range(-1,2): # we'll consider the putative sister in the target B-scan, as well as her left and right neighbors in the target B-scan
            tar_ascan = bscans[target_index][:,x+dx]
            xc = xcorr(ref_ascan,tar_ascan)
            correlations.append(np.max(xc)) # the max value of the cross-correlation xc is proportional to the Pearson correlation
            if dx==0: # keep track of the xc argmax, as it indicates the amount of whole-pixel shift between putative sisters
                shift = np.argmax(xc)
                if shift>len(xc)//2:
                    shift = shift - len(xc)
                peaks.append(shift)
        if not(np.argmax(correlations)==1): # if the A-scan isn't most correlated with its expected sister
            shifted_sister_count+=1
            #print(reference_index,'\t\t\t',shifted_sister_count,'\t\t\t',x,'\t\t\t\t',np.argmax(correlations)-1,'\t\t',['%0.2e'%c for c in correlations])
    print('Shifted sisters: %d of %d.'%(shifted_sister_count,n_fast))
    plt.subplot(1,2,reference_index+1)
    plt.plot(peaks)
    plt.title('axial shifts between sisters %d and %d'%(reference_index,target_index))
    plt.ylabel('shift')
    plt.xlabel('fast scan location')








dphase = np.angle(bscans[1][:,50])-np.angle(bscans[0][:,50])
coarse_bins = np.linspace(-np.pi*2,np.pi*2,10)
fine_bins = np.linspace(-np.pi*2,np.pi*2,200)
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.hist(dphase,bins=coarse_bins)
plt.title('coarse bins')
plt.subplot(1,2,2)
plt.hist(dphase,bins=fine_bins)
plt.title('fine bins')
plt.show()





def smoothed_histogram(data,n_bins,n_shifts,data_min,data_max):
    data_range = data_max-data_min
    bin_width = data_range/n_bins
    shift_amount = bin_width/n_shifts

    offsets = np.linspace(-bin_width/2.0,bin_width/2.0,n_shifts)
    base_bins = np.linspace(data_min-bin_width/2.0,data_max+bin_width/2.0,n_bins+1)

    all_bin_centers = []
    all_counts = []

    for offset in offsets:
        bins = base_bins+offset
        bin_centers = (bins[1:]+bins[:-1])/2.0
        all_bin_centers = all_bin_centers+list(bin_centers)
        all_counts = all_counts + list(np.histogram(data,bins)[0])

    order = np.argsort(all_bin_centers)
    all_bin_centers = np.array(all_bin_centers)
    all_counts = np.array(all_counts)
    all_bin_centers = all_bin_centers[order]
    all_counts = all_counts[order]

    return all_counts,all_bin_centers

n_bins = 8
n_shifts = 6
dphase_counts,dphase_values = smoothed_histogram(dphase,n_bins,n_shifts,-np.pi*2,np.pi*2)
plt.figure(figsize=(15,4))
plt.subplot(1,4,1)
plt.hist(dphase,bins=coarse_bins)
plt.title('coarse bins')
plt.subplot(1,4,2)
plt.hist(dphase,bins=fine_bins)
plt.title('fine bins')
plt.subplot(1,4,3)
plt.bar(dphase_values,dphase_counts)
plt.title('resampled bins')
plt.subplot(1,4,4)
plt.hist(dphase,bins=n_bins*n_shifts)
plt.title('corresponding simple histogram')
plt.show()





bscans = np.array(bscans)
print(bscans.shape)
bscans = np.transpose(bscans,(1,2,0))
print(bscans.shape)



def make_mask(im,threshold):
    mask = np.zeros(im.shape)
    mask[np.where(im>threshold)] = 1
    return mask

def centers_to_edges(bin_centers):
    # convert an array of bin centers to bin edges, using the mean
    # spacing of the centers to determine bin width

    # check if sorted:
    assert all(bin_centers[1:]>bin_centers[:-1])

    bin_width = np.mean(np.diff(bin_centers))
    half_width = bin_width/2.0
    first_edge = bin_centers[0]-half_width
    last_edge = bin_centers[-1]+half_width
    return np.linspace(first_edge,last_edge,len(bin_centers)+1)

def bin_shift_histogram(vals,bin_centers,resample_factor=1):
    shifts = np.linspace(bin_centers[0]/float(len(bin_centers)),
                          bin_centers[-1]/float(len(bin_centers)),resample_factor)

    n_shifts = len(shifts)
    n_bins = len(bin_centers)

    all_counts = np.zeros((n_shifts,n_bins))
    all_edges = np.zeros((n_shifts,n_bins+1))

    for idx,s in enumerate(shifts):
        edges = centers_to_edges(bin_centers+s)
        all_counts[idx,:],all_edges[idx,:] = np.histogram(vals,edges)

    all_centers = (all_edges[:,:-1]+all_edges[:,1:])/2.0
    all_counts = all_counts/float(resample_factor)
    all_centers = all_centers
    
    return all_counts.T.ravel(),all_centers.T.ravel()

def wrap_into_range(arr,phase_limits=(-np.pi,np.pi)):
    lower,upper = phase_limits
    above_range = np.where(arr>upper)
    below_range = np.where(arr<lower)
    arr[above_range]-=2*np.pi
    arr[below_range]+=2*np.pi
    return arr

def get_phase_jumps(phase_stack,mask,
                    n_bins=16,
                    resample_factor=24,
                    n_smooth=5,polynomial_smoothing=True):

    # Take a stack of B-scan phase arrays, with dimensions
    # (z,x,repeats), and return a bulk-motion corrected
    # version
    #phase_stack = np.transpose(phase_stack,(1,2,0))
    n_depth = phase_stack.shape[0]
    n_fast = phase_stack.shape[1]
    n_reps = phase_stack.shape[2]
    
    d_phase_d_t = np.diff(phase_stack,axis=2)
    d_phase_d_t = wrap_into_range(d_phase_d_t)

    d_phase_d_t = np.transpose(np.transpose(d_phase_d_t,(2,0,1))*mask,(1,2,0))
    bin_edges = np.linspace(-np.pi,np.pi,n_bins)
    
    # The key idea here is from Makita, 2006, where it is well explained. In
    # addition to using the phase mode, we also do bin-shifting, in order to
    # smooth the histogram. 
    b_jumps = np.zeros((d_phase_d_t.shape[1:]))
    bin_counts = np.zeros((d_phase_d_t.shape[1:]))

    for f in range(n_fast):
        valid_idx = np.where(mask[:,f])[0]
        for r in range(n_reps-1):
            vals = d_phase_d_t[valid_idx,f,r]
            
            [counts,bin_centers] = bin_shift_histogram(vals,bin_edges,resample_factor)
            bulk_shift = bin_centers[np.argmax(counts)]
            bin_count = np.max(counts)
            b_jumps[f,r] = bulk_shift
            bin_counts[f,r] = bin_count

    # Now unwrap to prevent discontinuities (although this may not impact complex variance)
    b_jumps = np.unwrap(b_jumps,axis=0)

    return b_jumps

def bulk_motion_correct(phase_stack,mask,
                        n_bins=16,
                        resample_factor=24,
                        n_smooth=5):

    # Take a stack of B-scan phase arrays, with dimensions
    # (z,x,repeats), and return a bulk-motion corrected
    # version

    n_reps = phase_stack.shape[2]

    b_jumps = get_phase_jumps(phase_stack,mask,
                              n_bins=n_bins,
                              resample_factor=resample_factor,
                              n_smooth=n_smooth)

    # Now, subtract b_jumps from phase_stack, not including the first repeat
    # Important: this is happening by broadcasting--it requires that the
    # last two dimensions of phase_stack[:,:,1:] be equal in size to the two
    # dimensions of b_jumps
    out = np.copy(phase_stack)

    errs = []
    for rep in range(1,n_reps):
        # for each rep, the total error is the sum of
        # all previous errors
        err = np.sum(b_jumps[:,:rep],axis=1)
        errs.append(err)
        out[:,:,rep] = out[:,:,rep]-err
    out = wrap_into_range(out)
    return out



average_bscan = np.mean(np.abs(bscans),axis=2)
mask = make_mask(average_bscan,np.percentile(average_bscan,90))
plt.imshow(mask)
plt.show()



bscans = bulk_motion_correct(bscans,mask)



